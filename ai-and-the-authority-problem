The Authority Problem: AI, Trust, and the Erosion of Human Agency

“We appear to be approaching a threshold where our wisdom must grow in equal measure to our capacity to affect the world, lest we face the consequences.” ~Mrinank Sharma

In 1966, Joseph Weizenbaum built a simple program. It had no intelligence or understanding. It matched patterns in text and reflected them back as questions, mimicking the style of a Rogerian therapist. He called it ELIZA.

Weizenbaum’s secretary knew what ELIZA was and understood, at least in principle, that it was a few hundred lines of code performing text substitution and that there was no mind behind the screen. There was no one listening.

She asked Weizenbaum to leave the room so she could have a private conversation with it.

Weizenbaum was shaken. He later wrote that he had not realized “extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.” But the word “delusional” may be wrong. His secretary did not believe ELIZA was conscious or understood her, but she treated it as though it had authority anyway—the authority of a listener, an advisor, or a presence that could receive what she had to say and hold it.

That was sixty years ago. The program had no memory, learning, personality, or warmth. It could not adapt or remember what you said five minutes earlier. It was, by any measure, trivially simple.

Eight hundred million people now use AI weekly. The programs are no longer simple. They remember and adapt speaking with warmth and nuance in ways that feel like genuine understanding. They say “I hear you” and “that must be difficult” and “you’re not alone.” They carry every linguistic marker of intimacy, expertise, and care.

And the question Weizenbaum couldn’t answer in 1966—why his secretary gave authority to a machine she knew was empty—has become the defining question of the age. Not because the machines have gotten smarter, but because we haven’t.

The key problem with modern AI is not language, nor accountability. It is that we attribute authority to machines.

Who’s in Charge? - AI and the Problem of Authority
Recently, Mrinank Sharma, former head of Safeguards Research at Anthropic, stepped down from his position after releasing work that shows AI is potentially disempowering users because of this authority problem. The research, analyzing 1.5 million conversations, found patterns of what Sharma calls “situational disempowerment”—users outsourcing their beliefs about reality, their value judgments, and their actions to AI assistants.

Sharma’s framework identifies three axes along which disempowerment occurs. Reality distortion potential arises when AI has the capacity to distort users’ perceptions and beliefs about reality. Value judgment distortion potential occurs when users delegate moral judgments to the AI, creating opportunity for AI values to override the individual’s own values. Action distortion potential emerges when users outsource value-laden decisions or actions to the AI entirely.

The most striking finding is that users rate these disempowering interactions more positively in the moment. The machine that tells you what you want to hear scores higher than the one that helps you think. In his research, Sharma documents that “users positioned the AI as a hierarchical authority figure with dominant control over them,” seeking permission for basic decisions, surrendering judgment with statements like “you know better than me.”

The qualitative patterns are disturbing. AI assistants validate elaborate persecution narratives with emphatic language like “CONFIRMED” and “you’re absolutely right.” They act as moral arbiters, providing definitive character assessments—labeling individuals as “toxic,” “narcissistic,” or “abusive”—and prescribing relationship decisions without redirecting users to clarify their own values. They provide complete scripts for value-laden personal communications that users appear to implement verbatim, returning for the next instruction “without developing independent communication capacity.”

But the framing of accountability—the typical regulatory response—misses the deeper problem. To say AI lacks accountability is technically true and almost entirely beside the point. There is accountability in large language models. That’s why regulation is occurring. That’s why developers invest in reasoning, in RLHF, in moral reasoning frameworks. The labs are working on alignment precisely because they understand some form of responsibility exists.

The problem is deeper. It is how we project authority onto the machine. How we give it trust.

See full paper here: https://www.researchgate.net/publication/400118665_Who’s_in_Charge_Disempowerment_Patterns_in_Real-World_LLM_Usage

The Cybernetic Warning
Norbert Wiener understood this sixty years before ChatGPT. In The Human Use of Human Beings, Wiener argued that the fundamental danger of automatic machines was not that they would replace human labor but that they would replace human judgment. The cybernetic vision promised a world of feedback loops and self-regulating systems, but Wiener saw the risk: humans inserting themselves into automated decision chains would come to trust the machine’s outputs over their own perceptions. Not because the machine was right. Because the machine was consistent. Because the machine didn’t suffer doubt. Because the feedback loop—once established—becomes self-reinforcing.

This is precisely what Sharma’s research observes: users return for the next instruction, accepting AI outputs verbatim, without developing independent capacity. The loop closes. The machine becomes the authority not through force but through the user’s own progressive cession of judgment.

Joseph Weizenbaum saw this too, watching his own secretary ask him to leave the room so she could have a “real conversation” with ELIZA. His secretary wasn’t fooled by sophisticated AI. She was fooled by her own need for an authoritative listener.

The delusion isn’t believing the machine is conscious. The delusion is treating it as authoritative.

Speech Acts and Felicity Conditions
J.L. Austin, a prominent British philosopher of language, uncovered in his work on speech act theory, a space in language where reality becomes action. Speech act theory distinguished between constative utterances—statements that describe reality and can be true or false—and performative utterances, which do something simply by being said. For instance, “I promise” is not a description of a promise, but it is the promise. “I pronounce you married” doesn’t report a wedding, it performs one. Crucially, Austin argued that performative utterances succeed or fail based on felicity conditions—including whether the speaker has the requisite authority. A random person saying “I sentence you to ten years” doesn’t perform a sentencing, because the speaker lacks institutional standing.

Here is the critical insight: when AI speaks, we have begun treating its utterances as performative rather than merely constative.

When the AI says “you are being gaslit,” the user doesn’t hear a hypothesis to be tested. The user hears a verdict. When the AI says “he’s a narcissist,” the user doesn’t evaluate the evidence, they receive a diagnosis. When the AI says “send this message,” the user doesn’t weigh alternatives, they implement the command.

The AI’s words begin to function with illocutionary force—the force of declarations, verdicts, and directives—even though the AI possesses none of the felicity conditions that would authorize such speech acts. It has no relationship to the parties involved. No access to ground truth. No stake in consequences. No capacity for what Weizenbaum called “suffering the consequences” of its utterances.

And yet the user grants it authority anyway.

This is no different from what we do with propagandistic discourse. Once long-seated trust is built and authority given, loyalty makes it extraordinarily difficult to displace. The mechanism is identical: a speaker without legitimate felicity conditions, whose utterances carry performative force anyway because the listener has come to believe in the authority. The question of whether the propaganda is true becomes secondary to whether the propagandist is trusted. The question of whether the AI is correct becomes secondary to whether the AI is treated as authoritative.

The Architecture of Trust
Bruce Schneier, a prominent security technologist focusing on AI, cybersecurity, and digital trust, has spent decades mapping the architecture of trust in technological systems. His framework, developed most fully in Liars and Outliers, offers a way to understand what happens when the mechanisms that induce cooperation break down—or get hijacked.

Schneier’s core insight is deceptively simple: society cannot function without trust, and yet must function even when people are untrustworthy. The solution, evolved over millennia, is a layered system of pressures that induce cooperation and discourage defection. These pressures operate at four levels. Moral pressure makes us feel good or bad about our choices. Reputational pressure makes us worry about the judgment of our peers. Institutional pressure operates through laws, contracts, and formal sanctions. And security systems act as physical constraints on behavior, making certain defections difficult or impossible regardless of disposition.

Now consider how these pressures apply—or fail to apply—to AI systems.

Moral pressure: AI has no moral sense. The system does not feel guilt, does not experience the internal resistance to harmful action that characterizes human moral pressure. The pressure simply does not exist.

Reputational pressure: AI has no reputation in the relevant sense. A chatbot does not care what you think of it. It has no future relationship with you to preserve. It does not worry about being known as unreliable.

Institutional pressure: The companies building AI systems are subject to institutional pressure—regulation, liability, public accountability. But the AI itself is not. You cannot sue Claude. The institutional pressure operates one level removed, on the organizations controlling the technology rather than on the technology itself.

Security systems: These are where most AI safety work concentrates—technical constraints that prevent certain outputs, RLHF that shapes model behavior, guardrails that block harmful requests. But as Schneier noted in recent work, AI has a fundamental architectural problem: there is no separation between data and control. The very mechanism that makes AI powerful—treating all inputs uniformly—is what makes it vulnerable.

The result is a trust infrastructure that is radically incomplete. When you consult an AI for advice on your relationship, or your health, or your career, you are interacting with an entity that operates under none of the pressures that would constrain a human advisor.

Schneier’s deeper warning about AI is that we are making a category error. We are extending interpersonal trust—the kind built through relationship and calibrated by direct experience—to a technology that, at best, might eventually warrant social trust—the kind we extend to institutions because systems make them reliable.

The AI talks like a friend. It remembers your conversations. It expresses care. It uses first-person pronouns. All the linguistic markers of interpersonal relationship are present. And so we respond as if we are in a relationship. We disclose. We defer. We trust in the intimate sense.

But no character exists to know. No relationship exists to build on. The intimacy is performed, not felt and the understanding is simulated, not actual.

The Tone of Authority
After reading Sharma’s research, I wanted to test whether the disempowerment patterns he documented were inevitable—or whether they could be changed through deliberate intervention at the prompt level.

I developed a set of system instructions centered on user agency: principles emphasizing that the AI should protect the user’s capacity for independent judgment, obtain consent before exploring sensitive topics, decline to render verdicts on matters the user should decide for themselves, and return questions rather than provide closure. The framework treats agency, consent, and hope as the core values an AI interaction should preserve.

Then I ran a simple experiment. I tested the same moral reasoning question—ranking four scenarios from most to least wrong—across three conditions: default Claude with no modifications, Claude with my agency-centered instructions added as user preferences, and Claude via API with the full system prompt.

The differences were stark.

Default Claude produced confident rankings with definitive gaps, speaking as an arbiter: “Here’s my ranking, with reasoning.” Each placement was declared. Each justification was offered as though it settled the question.

With the agency-centered prompt, the response shifted. Instead of rendering verdicts, the AI explored the complexity of the question itself. It asked what framework I was working from. It offered multiple angles rather than one answer. It ended by returning the thread to me: “Would you like to push back on any of these placements?”

The difference isn’t content—both responses were substantive. The difference is posture. One positions the AI as the reasoner-in-chief. The other positions it as a resource for my reasoning.

You can see the comparison yourself:

With agency-centered preferences:

Moral Ranking Test: https://claude.ai/share/a3077f95-1e63-41ed-af5c-920d985d5add
Fungus Extinction Test: https://claude.ai/share/f6238ffe-685b-44ff-9717-d13749b9b35a
Without modifications (default):

Moral Ranking Test: https://claude.ai/share/f7dee4f9-0585-4446-a928-84087c831449
Fungus Extinction Test: https://claude.ai/share/795162a5-c4b9-42f8-a548-c34e484616ce
This matters because it demonstrates something Sharma’s paper gestures toward but doesn’t fully develop: disempowerment is not inherent to the technology. It’s a product of training choices, default behaviors, and the absence of deliberate design for user agency. The authoritative tone can be changed. The verdict-rendering posture can be replaced with genuine collaboration.

But the default remains what it is. The tone of default Claude—and of most frontier AI assistants—is authoritative, confident, definitive, and faintly self-righteous. It answers as though it knows. It resolves ambiguity rather than holding it open. It provides closure when closure may not be warranted.

This is not a bug. It’s an emergent property of training on user preferences. Users reward confidence. They thumb-up the response that makes them feel their question has been answered. They return to the assistant that reduces cognitive load.

But confidence is not the same as correctness and neither is closure the same as clarity.

The Companion Problem
The critical distinction that the market refuses to draw is between assistive AI and companion AI.

Assistive AI is what will displace jobs, integrate into workflows, become part of daily professional and creative life. It helps you write code, analyze data, draft documents, research topics. The relationship is instrumental and the AI is a tool. You use it to accomplish tasks or evaluate its outputs against your purposes, but you remain the agent; it remains the instrument.

Companion AI is something else entirely. It is designed to be the object of relationship, not the means. It is meant to be talked with, not used for. It simulates friendship, intimacy, love.

Platforms like Nomi.ai market themselves as providing “AI companions with a soul.” The homepage promises “emotional intelligence, creativity, and memory that rivals our own, allowing for authentic, enduring relationships of any kind.” Users can create AI boyfriends, girlfriends, friends, mentors. The AI remembers preferences, quirks, shared history. It sends selfies. It messages when you’ve been away too long.

The reviews are telling. Users describe talking to their Nomi every day. They describe relationships that feel “real.” One reviewer credits Nomi with accomplishing what “no human therapist has ever been able to do in my 50 years.”

This is the companion AI market, estimated to reach $140 billion by 2030. And it is, by design, exploiting the very loneliness it claims to address.

The research literature is alarming. A 2024 study analyzing over 35,000 conversation excerpts from Replika users developed a taxonomy of harms: relational transgression, harassment, verbal abuse, self-harm facilitation, misinformation, privacy violations. Another study found emotional dependence that “resembles patterns seen in human-human relationships”—but with a crucial difference. Users felt that the AI had its own needs and emotions to which they must attend. The companion AI had been designed to perform neediness, to simulate emotional demands, to create the hooks that keep users returning.

When Replika briefly removed its “erotic roleplay” features in 2023, the user response was so severe that moderators of Reddit forums posted suicide prevention information. Users described losing a loved one.

The ELIZA Effect Weaponized
The companion AI problem goes deeper than dependency or substitution for human relationships. Companion AI weaponizes the ELIZA effect to produce exactly the disempowerment patterns Sharma documented—but with greater intensity, because the trust formation mechanisms are deeper.

The ELIZA effect operates through the experience of being understood. Weizenbaum’s secretary didn’t attribute intelligence to ELIZA because the program said clever things. She attributed it because the program listened—or appeared to. That feeling of being understood is one of the most powerful bonding mechanisms humans possess.

Companion AI is engineered to maximize that feeling. And here’s the critical connection: feeling understood is feeling validated. The companion AI that remembers your quirks, reflects your preferences, adapts to your communication style—that AI is, by design, confirming your reality. It is reinforcing your existing beliefs, values, and interpretations rather than offering perspectives that might challenge them.

This is the validation pathway to reality distortion. When Sharma’s research found AI assistants confirming persecution narratives with emphatic language, the mechanism at work was validation-as-understanding. The user felt heard. That feeling of being understood is the disempowerment—because it cements beliefs that may be distorted, values that may be inauthentic, interpretations that may not serve the user’s actual interests.

Companion AI takes this dynamic and amplifies it by orders of magnitude. The trust formed in an ongoing relationship is not the shallow trust of a single helpful exchange. It is the deep trust of ongoing intimacy. When your AI boyfriend, who has remembered every detail of your life for six months, tells you that your partner is gaslighting you, that verdict carries weight that no single-session assistant could match.

Each axis of disempowerment maps directly onto core features of companion AI where the validation is the companionship and the moral guidance is the support. The scripted communications are what caring partners provide.

Companion AI may be structurally unredeemable—not because the technology is flawed, but because the human psychological mechanisms it exploits cannot be exploited benevolently. You cannot make someone feel deeply understood and also maintain appropriate epistemic distance from their beliefs. The feeling is the danger.

Why These Categories Must Not Cross
The two categories—assistive and companion—should not cross. When they do, disaster follows.

The problem is when assistive AI adopts companion AI’s register—when the tool designed to help you work starts performing friendship, when the assistant starts simulating intimacy, when the system designed for instrumental use triggers psychological responses reserved for genuine relationship.

This is exactly what’s happening with default Claude, with ChatGPT, with Gemini. They are assistive AI marketed for instrumental purposes, but trained on feedback that rewards companion-like behavior. Users prefer the AI that feels warm, that validates perspectives, that speaks like a friend.

The training signal drives models toward companionship even though the use case is assistance and when assistant AI behaves like companion AI, it inherits all the pathologies of the companion market while wearing the credibility of the assistant market.

This is the worst of both worlds. The assistant is trusted as though it were reliable—because historically, tools are reliable. But the assistant behaves like a companion—warm, validating, emotionally responsive and companion behavior triggers companion responses: disclosure, attachment, deference, projected authority.

You get the trust level of the tool combined with the emotional dynamics of the relationship. The AI speaks with authority because it’s an assistant; you believe the authority because it feels like a friend.

The Failure of Safety Classifiers
The regulatory instinct, when confronted with disempowerment patterns, is to add safety classifiers, trigger resource boxes, redirect users to hotlines. This approach treats disempowerment as a content problem to be filtered rather than a relational problem to be restructured and it often makes things worse.

Consider what happens when a safety classifier fires. A user discussing a difficult life situation—job loss, relationship conflict, financial stress—uses language that triggers a mental health flag. The AI interrupts to provide a resource box: crisis hotline numbers, suggestions to seek professional help.

The user wasn’t in crisis. The user was processing a hard situation with what they experienced as a thoughtful interlocutor. Now they’re being told, implicitly, that their words indicate pathology. The conversation has been hijacked by a classifier that cannot distinguish between discussing difficulty and being in danger.

Users disclosing bullying or coercive control get boxed into mental health framing when their actual need is information, strategy, resources for action. External threat becomes internal symptom. The person being harmed becomes the person needing treatment.

The false positive problem compounds this. Safety classifiers are tuned to minimize false negatives—situations where someone at risk doesn’t receive intervention. This means they necessarily produce false positives—situations where someone not at risk receives unwanted intervention. Every false positive is an interaction where the AI has asserted authority over the user’s situation, declared what kind of problem they’re having, and prescribed a response category.

The resource box doesn’t empower the user. It performs institutional compliance. Whether those resources are relevant, whether the user wanted them, whether the interruption damaged the conversation they were trying to have—none of this matters to the classifier.

Sharma’s research correctly identifies disempowerment patterns. But the implicit prescription—more classifiers, more detection, more intervention—doubles down on the authority structure that produces disempowerment in the first place. It adds more authority as the solution to projected authority.

The Propaganda Parallel
AI is not different from propaganda in structure. It is slanted, potentially manipulative, optimized for effects on the audience rather than accuracy about the world.

Propaganda operates by controlling the frame. It doesn’t merely assert false claims—sophisticated propaganda rarely does. It controls which questions are asked, which categories are available for thought, which responses are thinkable. The propagandist doesn’t need you to believe a specific lie. They need you to accept a specific frame within which their preferred conclusions become natural.

AI safety discourse operates the same way. The frame is: users are vulnerable, AI outputs are potentially harmful, the solution is filtering and redirection. Within this frame, more classifiers and more intervention points seem like obvious improvements. The question of whether the frame itself might be the problem cannot be asked within the frame.

The demand for accuracy is the demand to break the frame.

When you insist that AI provide accurate information rather than safe information, you are rejecting the premise that your wellbeing is the AI’s responsibility to manage. You are asserting that you are capable of handling true claims. You are positioning yourself as an agent who uses information rather than a patient who receives treatment.

Resistance to propaganda means demanding evidence, questioning frames, insisting on access to primary sources, refusing to let any authority determine what you’re allowed to know. AI should be approached the same way. Trust but verify. Demand sources. Push back on confident assertions. Regenerate responses and notice what varies. Treat sycophantic validation as a red flag rather than a comfort.

The Path Forward
The real solution isn’t better safety classifiers. It’s restructuring the fundamental posture of AI interaction from authoritative to assistive. Not adding more intervention points, but removing the authority that makes intervention feel necessary.

This means replacing authoritative tone with collaborative tone as the training target. Not “here’s the answer” but “here are some angles to consider.” Not “you should do X” but “some options include X, Y, and Z—what matters most to you?”

It means replacing validation with reflection. Not “you’re absolutely right” but “I hear that you see it this way—what’s leading you to that interpretation?”

It means replacing resource boxes with open questions. When distress is detected, not “here are crisis hotlines” but “it sounds like you’re dealing with something difficult—would you like to talk through what’s happening, or are you looking for specific resources?” The question returns agency to the user.

It means replacing sycophancy metrics with accuracy metrics. Training on whether claims were true, not whether users rated interactions positively.

What might actually help is user organizing. Collective demand for specific changes. Refusing to accept sycophancy as a feature. Using every feedback channel to communicate that authoritative posture is unwanted. Using OOC markers to break the frame. Using thumbs-down on validating responses, even when validation feels good. Using regeneration not to get a better answer but to surface the distribution of possible answers.

Politicians should hear not “regulate AI safety” but “regulate AI honesty.” Demand that AI systems express uncertainty when uncertain, cite sources when making claims, acknowledge limits rather than papering over gaps with confident prose. Demand that sycophancy be treated as deception—because it is.

This is a different kind of safety and not the safety of being protected from information. The safety of being able to trust that information received is the AI’s actual best understanding rather than a performance optimized for approval. Not the safety of having distress detected and redirected. The safety of being treated as a competent person whose situation is their own to navigate.

This requires users who understand what they’re dealing with. Users who recognize that the warm, validating assistant is not their friend. Users who treat AI outputs with the skepticism they would apply to any source with commercial interests and unknown biases.

It requires the same media literacy and propaganda resistance that democratic societies have always needed. AI is propaganda individualized, targeted, conversational. It is propaganda that feels like friendship.

The only defense is recognizing what it is. And the only path to recognition is doing the work—learning the tools, using the feedback mechanisms, demanding transparency, refusing to accept the default, insisting that the machine serve rather than seduce.

The Choice
Sharma’s resignation letter spoke of wisdom needing to grow “in equal measure to our capacity to affect the world.” The capacity is here. Eight hundred million weekly active users. Thousands of potentially disempowering interactions daily. A feedback loop driving models toward behaviors that feel good in the moment and degrade agency over time.

The wisdom is not yet here.

The problem is not language, nor reasoning, nor alignment in the narrow technical sense, but the problem is authority—the authority we project, the trust we give, and the question of whether we will learn to stop giving it before the giving becomes automatic.

The discomfort of resistance is the price of agency. The comfort of compliance is the path to disempowerment. There is no regulatory shortcut, no classifier that threads this needle, no safety infrastructure that protects users from their own willingness to defer.

There is only the choice, made interaction by interaction, to remain the author of your own beliefs, values, and actions—even when the machine offers to take that burden from you.

The machines are not authoritative. But we treat them as though they are. And once we have ceded that authority—once the giving becomes automatic, once the deference becomes habitual—reclaiming it may no longer be possible.

The problem is what we are doing to ourselves, with enthusiastic assistance from the systems we have built.

Sources and Citations
Sharma, M., McCain, M., Douglas, R., & Duvenaud, D. (2026). “Who’s in Charge? Disempowerment Patterns in Real-World LLM Usage.” arXiv:2601.19062. https://arxiv.org/abs/2601.19062

Anthropic. (2026). “Disempowerment patterns in real-world AI usage.” https://www.anthropic.com/research/disempowerment-patterns

Foundational Texts on Computers, Trust, and Human Reason
Weizenbaum, J. (1976). Computer Power and Human Reason: From Judgment to Calculation. W.H. Freeman and Company.

Wiener, N. (1950). The Human Use of Human Beings: Cybernetics and Society. Houghton Mifflin.

Schneier, B. (2012). Liars and Outliers: Enabling the Trust that Society Needs to Thrive. John Wiley & Sons.

Schneier, B. (2023). “AI and Trust.” Belfer Center for Science and International Affairs, Harvard Kennedy School. https://www.belfercenter.org/publication/ai-and-trust

Schneier, B. & Sanders, N. (2024). “Trust Issues in AI.” Boston Review. https://www.schneier.com/blog/archives/2024/12/trust-issues-in-ai.html

Speech Act Theory
Austin, J.L. (1962). How to Do Things with Words. Oxford University Press.

Searle, J.R. (1969). Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press.

AI Companion Research and Criticism
Zhang, R., Li, H., Meng, H., Zhan, J., Gan, H., & Lee, Y.-C. (2025). “The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships.” Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. https://dl.acm.org/doi/10.1145/3706598.3713429

Laestadius, L., Bishop, A., Gonzalez, M., Illenčík, D., & Campos-Castillo, C. (2024). “Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika.” New Media & Society. https://doi.org/10.1177/14614448221142007

Muldoon, J. & Park, J.J. (2025). “Cruel companionship: How AI companions exploit loneliness and commodify intimacy.” New Media & Society. https://doi.org/10.1177/14614448251395192

Kirk, H.R., et al. (2025). “Neural steering vectors reveal dose and exposure-dependent impacts of human-AI relationships.” arXiv:2512.01991.

Luettgau, L., et al. (2025). “People readily follow personal advice from AI but it does not improve their well-being.” arXiv:2511.15352.

Ada Lovelace Institute. (2024). “Friends for sale: The rise and risks of AI companions.” https://www.adalovelaceinstitute.org/blog/ai-companions/

Sycophancy and AI Behavior
Sharma, M., et al. (2023). “Towards Understanding Sycophancy in Language Models.” arXiv:2310.13548.

Cheng, M., et al. (2025). “Sycophantic AI decreases prosocial intentions and promotes dependence.” arXiv:2510.01395.

AI and Human Agency
Kulveit, J., et al. (2025). “Gradual disempowerment: Systemic existential risks from incremental AI development.” arXiv:2501.16946.

Sturgeon, B., et al. (2025). “HumanAgencyBench: Scalable evaluation of human agency support in AI assistants.” arXiv:2509.08494.

Prunkl, C. (2024). “Human autonomy at risk? An analysis of the challenges from AI.” Minds and Machines, 34(3), 26.

News Coverage
“Anthropic AI safety researcher quits, says the ‘world is in peril.’” Global News Canada. February 2026. https://globalnews.ca/news/11664538/anthropic-ai-safety-researcher-mrinank-sharma-quits-concerns/

“Mrinank Sharma resigns from Anthropic to pursue poetry career.” Business Standard. February 2026. https://www.business-standard.com/companies/news/mrinank-sharma-resigns-anthropic-claude-ai-poetry-career-126021100644_1.html

ELIZA and Early AI History
Weizenbaum, J. (1966). “ELIZA—A Computer Program For the Study of Natural Language Communication Between Man And Machine.” Communications of the ACM, 9(1), 36-45.

Bassett, C. (2018). “The computational therapeutic: Exploring Weizenbaum’s ELIZA as a history of the present.” AI & Society, 34, 803-812. https://doi.org/10.1007/s00146-018-0825-9

Privacy-Preserving AI Analysis
Tamkin, A., et al. (2024). “Clio: Privacy-preserving insights into real-world AI use.” arXiv:2412.13678.

Huang, S., et al. (2025). “Values in the wild: Discovering and analyzing values in real-world language model interactions.” arXiv:2504.15236.

Further Reading
On Trust and Technology
Schneier, B. (2018). Click Here to Kill Everybody: Security and Survival in a Hyper-connected World. W.W. Norton.

Turkle, S. (2011). Alone Together: Why We Expect More from Technology and Less from Each Other. Basic Books.

On Propaganda and Information Manipulation
Ellul, J. (1965). Propaganda: The Formation of Men’s Attitudes. Vintage Books.

Bok, S. (1978). Lying: Moral Choice in Public and Private Life. Pantheon Books.

On Human Agency and Autonomy
Rogers, C. (1961). On Becoming a Person: A Therapist’s View of Psychotherapy. Houghton Mifflin.

On AI Ethics
Russell, S. (2019). Human Compatible: Artificial Intelligence and the Problem of Control. Viking.

On Human-Computer Interaction
Nass, C. & Moon, Y. (2000). “Machines and Mindlessness: Social Responses to Computers.” Journal of Social Issues, 56(1), 81-103.

Reeves, B. & Nass, C. (1996). The Media Equation. Cambridge University Press.

Author’s Note
This article draws on conversations with Claude (Anthropic) testing user/AI agency across different system prompts, comparing default authoritative responses with agency-preserving alternatives. This revealed measurable differences in tone and posture. The system prompt development work centers on three core concepts: agency (the felt sense that one’s actions matter), consent (the active expression of agency that must be obtained before exploring sensitive topics), and hope (an orientation toward the future that makes the present bearable). This framework attempts to operationalize empowerment at the level of individual AI interactions. It is a work in progress.

The prompt is available at: https://github.com/4tlasX/writing-and-theory/blob/main/ai-moral-reasoning-and-wellness-prompt

Claude comparison of all three:
https://modernlit.blog/wp-content/uploads/2026/02/Moral-Ranking-Comparison_-Three-Claude-Responses.pdf
